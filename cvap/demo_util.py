

import torch
import clip
import subprocess
from pathlib import Path
import os
import cv2
import numpy as np
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
from tqdm import tqdm
import librosa
import torch
from omegaconf import OmegaConf
import importlib


def which_ffmpeg() -> str:
    '''Determines the path to ffmpeg library

    Returns:
        str -- path to the library
    '''
    result = subprocess.run(['which', 'ffmpeg'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ffmpeg_path = result.stdout.decode('utf-8').replace('\n', '')
    return ffmpeg_path


def reencode_video_with_diff_fps(video_path: str, tmp_path: str, extraction_fps: int, start_second, truncate_second) -> str:
    '''Reencodes the video given the path and saves it to the tmp_path folder.

    Args:
        video_path (str): original video
        tmp_path (str): the folder where tmp files are stored (will be appended with a proper filename).
        extraction_fps (int): target fps value

    Returns:
        str: The path where the tmp file is stored. To be used to load the video from
    '''
    assert which_ffmpeg() != '', 'Is ffmpeg installed? Check if the conda environment is activated.'
    # assert video_path.endswith('.mp4'), 'The file does not end with .mp4. Comment this if expected'
    # create tmp dir if doesn't exist
    os.makedirs(tmp_path, exist_ok=True)

    # form the path to tmp directory
    if truncate_second is None:
        new_path = os.path.join(tmp_path, f'{Path(video_path).stem}_new_fps_{str(extraction_fps)}.mp4')
        cmd = f'{which_ffmpeg()} -hide_banner -loglevel panic '
        cmd += f'-y -i {video_path} -an -filter:v fps=fps={extraction_fps} {new_path}'
        subprocess.call(cmd.split())
    else:
        new_path = os.path.join(tmp_path, f'{Path(video_path).stem}_new_fps_{str(extraction_fps)}_truncate_{start_second}_{truncate_second}.mp4')
        cmd = f'{which_ffmpeg()} -hide_banner -loglevel panic '
        cmd += f'-y -ss {start_second} -t {truncate_second} -i {video_path} -an -filter:v fps=fps={extraction_fps} {new_path}'
        subprocess.call(cmd.split())
    return new_path


def instantiate_from_config(config):
    if not "target" in config:
        if config == '__is_first_stage__':
            return None
        elif config == "__is_unconditional__":
            return None
        raise KeyError("Expected key `target` to instantiate.")
    return get_obj_from_str(config["target"])(**config.get("params", dict()))


def get_obj_from_str(string, reload=False):
    module, cls = string.rsplit(".", 1)
    if reload:
        module_imp = importlib.import_module(module)
        importlib.reload(module_imp)
    return getattr(importlib.import_module(module, package=None), cls)



class Extract_CAVP_Features(torch.nn.Module):

    def __init__(self, fps=4, batch_size=2, device=None, tmp_path="./", video_shape=(224,224), config_path=None, ckpt_path=None):
        super(Extract_CAVP_Features, self).__init__()
        self.fps = fps
        self.batch_size = batch_size
        self.device = device
        self.tmp_path = tmp_path

        # Initalize Stage1 CAVP model:
        print("Initalize Stage1 CAVP Model")
        config = OmegaConf.load(config_path)
        self.stage1_model = instantiate_from_config(config.model).to(device)

        # Loading Model from:
        assert ckpt_path is not None
        print("Loading Stage1 CAVP Model from: {}".format(ckpt_path))
        self.init_first_from_ckpt(ckpt_path)
        self.stage1_model.eval()
        
        # Transform:
        self.img_transform = transforms.Compose([
            transforms.Resize(video_shape),
            transforms.ToTensor(),
        ])
    
    
    def init_first_from_ckpt(self, path):
        model = torch.load(path, map_location="cpu")
        if "state_dict" in list(model.keys()):
            model = model["state_dict"]
        # Remove: module prefix
        new_model = {}
        for key in model.keys():
            new_key = key.replace("module.","")
            new_model[new_key] = model[key]
        missing, unexpected = self.stage1_model.load_state_dict(new_model, strict=False)
        print(f"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys")
        if len(missing) > 0:
            print(f"Missing Keys: {missing}")
        if len(unexpected) > 0:
            print(f"Unexpected Keys: {unexpected}")
    
    
    @torch.no_grad()
    def forward_from_mp4(self, video_path, device, start_second=None, truncate_second=None, tmp_path="./tmp_folder"):
        device = device
        self.stage1_model.to(device)
        self.tmp_path = tmp_path
        
        # print("video_path", video_path)
        # print("truncate second: ", truncate_second)
        # Load the video, change fps:
        video_path_low_fps = reencode_video_with_diff_fps(video_path, self.tmp_path, self.fps, start_second, truncate_second)
        video_path_high_fps = reencode_video_with_diff_fps(video_path, self.tmp_path, 21.5, start_second, truncate_second)
        
        # read the video:
        cap = cv2.VideoCapture(video_path_low_fps)

        feat_batch_list = []
        video_feats = []
        first_frame = True
        # pbar = tqdm(cap.get(7))
        i = 0
        while cap.isOpened():
            i += 1
            # pbar.set_description("Processing Frames: {} Total: {}".format(i, cap.get(7)))
            frames_exists, rgb = cap.read()
            
            if first_frame:
                if not frames_exists:
                    continue
            first_frame = False

            if frames_exists:
                rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
                rgb_tensor = self.img_transform(Image.fromarray(rgb)).unsqueeze(0).to(device)
                feat_batch_list.append(rgb_tensor)      # 32 x 3 x 224 x 224
                
                # Forward:
                if len(feat_batch_list) == self.batch_size:
                    # Stage1 Model:
                    input_feats = torch.cat(feat_batch_list,0).unsqueeze(0).to(device)
                    contrastive_video_feats = self.stage1_model.encode_video(input_feats, normalize=True, pool=False)
                    video_feats.extend(contrastive_video_feats.detach().cpu().numpy())
                    feat_batch_list = []
            else:
                if len(feat_batch_list) != 0:
                    input_feats = torch.cat(feat_batch_list,0).unsqueeze(0).to(device)
                    contrastive_video_feats = self.stage1_model.encode_video(input_feats, normalize=True, pool=False)
                    video_feats.extend(contrastive_video_feats.detach().cpu().numpy())
                cap.release()
                break
        try:
            video_contrastive_feats = np.concatenate(video_feats)
        except:
            print(video_path)
            import pdb; pdb.set_trace()
        if video_contrastive_feats.shape[0] < self.batch_size:
            pad = np.zeros((self.batch_size - video_contrastive_feats.shape[0], video_contrastive_feats.shape[1]))
            video_contrastive_feats = np.concatenate([video_contrastive_feats, pad], axis=0)
        elif video_contrastive_feats.shape[0] > self.batch_size:
            video_contrastive_feats = video_contrastive_feats[:self.batch_size]
        return video_contrastive_feats, video_path_high_fps

    @torch.no_grad()
    def forward(self, video_frame_path, device, start_second=None, truncate_second=None):
        device = device
        self.stage1_model.to(device)
        
        video_tensor = torch.load(video_frame_path).to(device)

        feat_batch_list = []
        video_feats = []
        min_len = min(video_tensor.size(0), self.batch_size)
        for rgb_tensor in video_tensor: 
            # import pdb; pdb.set_trace()
            rgb_tensor = rgb_tensor.unsqueeze(0).to(device)
            feat_batch_list.append(rgb_tensor)      # 32 x 3 x 224 x 224
            # Forward:
            if len(feat_batch_list) == min_len:
                # import pdb; pdb.set_trace()
                # Stage1 Model:
                input_feats = torch.cat(feat_batch_list,0).unsqueeze(0).to(device)
                contrastive_video_feats = self.stage1_model.encode_video(input_feats, normalize=True, pool=False)
                video_feats.extend(contrastive_video_feats.detach().cpu().numpy())
                feat_batch_list = []
                continue

        # if len(feat_batch_list) != 0:
        #     input_feats = torch.cat(feat_batch_list,0).unsqueeze(0).to(device)
        #     contrastive_video_feats = self.stage1_model.encode_video(input_feats, normalize=True, pool=False)
        #     video_feats.extend(contrastive_video_feats.detach().cpu().numpy())
        
        video_contrastive_feats = np.concatenate(video_feats)
        if video_contrastive_feats.shape[0] < self.batch_size:
            pad = np.zeros((self.batch_size - video_contrastive_feats.shape[0], video_contrastive_feats.shape[1]))
            video_contrastive_feats = np.concatenate([video_contrastive_feats, pad], axis=0)
        elif video_contrastive_feats.shape[0] > self.batch_size:
            video_contrastive_feats = video_contrastive_feats[:self.batch_size]
        return video_contrastive_feats


def load_model_from_config(config, ckpt, verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=False)
    # if len(m) > 0 and verbose:
    #     print("missing keys:")
    #     print(m)
    # if len(u) > 0 and verbose:
    #     print("unexpected keys:")
    #     print(u)
    model.cuda()
    model.eval()
    return model


def inverse_op(spec):
    sr = 22050
    n_fft = 1024
    fmin = 125
    fmax = 7600
    nmels = 80
    hoplen = 1024 // 4
    spec_power = 1

    # Inverse Transform
    spec = spec * 100 - 100
    spec = (spec + 20) / 20
    spec = 10 ** spec
    spec_out = librosa.feature.inverse.mel_to_stft(spec, sr = sr, n_fft = n_fft, fmin=fmin, fmax=fmax, power=spec_power)
    wav = librosa.griffinlim(spec_out, hop_length=hoplen)
    return wav


